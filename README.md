# ğŸ¤– AI News Aggregator

An intelligent, automated news aggregation system that collects AI-related content from multiple sources, processes it with AI, and delivers a curated daily newsletter directly to your inbox.

## ğŸ“– What is This Project?

**AI News Aggregator** is a Python-based automation system that:
- **Scrapes** AI news from top sources (YouTube channels, OpenAI blog, Anthropic research, Formula 1 AI content)
- **Processes** content using AI to extract transcripts, convert markdown, and generate summaries
- **Curates** the most relevant articles using AI-powered ranking
- **Delivers** a beautifully formatted daily email digest with featured articles and additional links

Perfect for staying up-to-date with the latest developments in AI without manually checking multiple sources every day.

---

## ğŸ¯ What Does This Project Do?

### Core Functionality

1. **Multi-Source Content Collection**
   - ğŸ¬ **YouTube**: Scrapes videos from configured AI-focused channels
   - ğŸ¤– **OpenAI**: Fetches blog posts and announcements from OpenAI
   - ğŸ§  **Anthropic**: Collects research papers and engineering updates from Anthropic
   - ğŸï¸ **Formula 1**: Gathers AI-related Formula 1 content

2. **Intelligent Content Processing**
   - Extracts and processes YouTube video transcripts
   - Converts Anthropic markdown articles to readable format
   - Stores all content in PostgreSQL database for efficient retrieval

3. **AI-Powered Summarization**
   - Uses Google Gemini API to generate concise, informative summaries
   - Creates compelling titles and 2-3 sentence summaries for featured articles
   - Focuses on actionable insights and key implications

4. **Smart Curation & Ranking**
   - AI-powered curator ranks articles by relevance and importance
   - Selects top articles from each source for featured section
   - Includes additional links grouped by source

5. **Automated Email Delivery**
   - Generates beautifully formatted HTML email newsletters
   - Includes personalized introduction using AI
   - Sends daily digest to your email address

---

## ğŸ”„ Project Flow

### Daily Pipeline Execution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DAILY PIPELINE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 0: ğŸ—‘ï¸  Cleanup
â”œâ”€ Deletes data older than 7 days
â””â”€ Keeps database size manageable

Step 1: ğŸ“¡ Scraping
â”œâ”€ YouTube: Fetch latest videos from configured channels
â”œâ”€ OpenAI: Get recent blog posts from RSS feed
â”œâ”€ Anthropic: Collect articles from research/engineering feeds
â””â”€ Formula 1: Gather AI-related F1 content

Step 2: ğŸ“„ Processing
â”œâ”€ Anthropic: Convert markdown articles to readable format
â””â”€ YouTube: Extract and process video transcripts

Step 3: ğŸ¤– AI Digests
â”œâ”€ Select top 1 article from each source (past 24 hours)
â”œâ”€ Generate AI summaries using Gemini API
â””â”€ Create compelling titles and summaries

Step 4: ğŸ“§ Newsletter Generation
â”œâ”€ Rank articles by relevance (AI-powered curator)
â”œâ”€ Generate personalized email introduction
â”œâ”€ Format featured articles with AI summaries
â”œâ”€ Add additional links grouped by source
â””â”€ Send beautiful HTML email to MY_EMAIL

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RESULT                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“¬ Daily email digest delivered to your inbox!
```

### Newsletter Format

Each email includes:

- **ğŸ¬ YOUTUBE** - Featured video with AI-generated summary
- **ğŸ¤– OPENAI** - Featured blog post with AI summary
- **ğŸ§  ANTHROPIC** - Featured research/update with AI summary
- **ğŸï¸ F1** - Featured Formula 1 AI content with summary
- **ğŸ“š MORE ARTICLES** - Additional links organized by source

---

## ğŸ—ï¸ Architecture

### Technology Stack

- **Language**: Python 3.12+
- **Database**: PostgreSQL (SQLAlchemy ORM)
- **AI API**: Google Gemini (for summaries and curation)
- **Email**: SMTP (Gmail)
- **Web Framework**: Flask (for API endpoints)
- **Scraping**: RSS feeds, YouTube API, feedparser

### Key Components

```
app/
â”œâ”€â”€ scrapers/          # Content scrapers for each source
â”‚   â”œâ”€â”€ youtube.py
â”‚   â”œâ”€â”€ openai.py
â”‚   â”œâ”€â”€ anthropic.py
â”‚   â””â”€â”€ formula1.py
â”œâ”€â”€ services/          # Processing services
â”‚   â”œâ”€â”€ process_anthropic.py
â”‚   â”œâ”€â”€ process_youtube.py
â”‚   â”œâ”€â”€ process_digest.py
â”‚   â””â”€â”€ process_email.py
â”œâ”€â”€ agent/             # AI agents
â”‚   â”œâ”€â”€ digest_agent.py    # Generates summaries
â”‚   â”œâ”€â”€ curator_agent.py    # Ranks articles
â”‚   â””â”€â”€ email_agent.py      # Creates email content
â”œâ”€â”€ database/          # Database layer
â”‚   â”œâ”€â”€ models.py          # SQLAlchemy models
â”‚   â”œâ”€â”€ repository.py      # Data access layer
â”‚   â””â”€â”€ connection.py      # Database connection
â””â”€â”€ daily_runner.py    # Main pipeline orchestrator
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.12+
- PostgreSQL database
- Google Gemini API key
- Gmail account with app password

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/ai-news-aggregator.git
   cd ai-news-aggregator
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   # or using uv
   uv sync
   ```

3. **Set up environment variables**
   ```bash
   cp app/example.env .env
   # Edit .env with your credentials
   ```

4. **Configure database**
   ```bash
   # Start PostgreSQL (using Docker)
   cd docker && docker-compose up -d && cd ..
   
   # Create tables
   python app/database/create_tables.py
   ```

5. **Run the pipeline**
   ```bash
   python main.py
   ```

---

## ğŸ“Š Features

- âœ… **Automated Daily Execution** - Runs automatically via cron job
- âœ… **Multi-Source Aggregation** - Collects from 4+ sources
- âœ… **AI-Powered Summaries** - Uses Gemini for intelligent summarization
- âœ… **Smart Curation** - AI ranks articles by relevance
- âœ… **Beautiful Email Format** - Professional HTML newsletter design
- âœ… **Data Retention** - Automatically cleans up old data (7-day retention)
- âœ… **API Endpoints** - Flask API for manual triggers and health checks
- âœ… **Error Handling** - Robust error handling and logging

---

## ğŸ”§ Configuration

Key configuration options (in `app/api_config.py`):

- `SCRAPE_HOURS`: How far back to scrape (default: 168 hours / 7 days)
- `NEWSLETTER_HOURS`: Articles to include in newsletter (default: 24 hours)
- `DATA_RETENTION_HOURS`: How long to keep data (default: 168 hours / 7 days)
- `TOP_PER_SOURCE`: Featured articles per source (default: 1)
- `ADDITIONAL_LINKS_PER_SOURCE`: Extra links per source (default: 5)

---

## ğŸ“ Environment Variables

Required environment variables:

```env
# AI API
GEMINI_API_KEY=your_gemini_api_key

# Email Configuration (Resend for Render; Gmail SMTP for local)
MY_EMAIL=your_email@gmail.com
RESEND_API_KEY=re_xxx    # From resend.com/api-keys (required on Render)
FROM_EMAIL=onboarding@resend.dev
# APP_PASSWORD=xxx       # Optional: Gmail app password for local SMTP

# Database (local development)
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=ai_news_aggregator

# Or use DATABASE_URL for cloud deployment
# DATABASE_URL=postgresql://user:password@host:port/database
```

---

## ğŸŒ Deployment

The project includes deployment configuration for **Render**:

- **Database**: PostgreSQL (free tier)
- **Web Service**: Flask API with `/health` and `/trigger-newsletter` endpoints
- **Cron Job**: Automated daily execution

See `RENDER_DEPLOYMENT_GUIDE.md` for detailed deployment instructions.

---

## ğŸ“ˆ API Usage

The system uses approximately **5 API calls per run**:
- 4 calls for AI digests (1 per source)
- 1 call for email introduction/curation

This keeps costs low while providing intelligent summarization.

---

## ğŸ—ºï¸ Roadmap & Future Plans

### Current Status
âœ… **Phase 1: Core Functionality** - Complete
- Multi-source content aggregation
- AI-powered summarization
- Automated email delivery
- Local development setup

### Next Steps

**ğŸš€ Phase 2: Deployment** (In Progress)
- Cloud deployment on Render
- Production-ready infrastructure
- Automated daily execution via cron jobs
- API endpoints for manual triggers

**ğŸ‘¥ Phase 3: Multi-Subscriber Support** (Planned)
- User subscription management system
- Customizable newsletter preferences
- **Source Selection**: Subscribers can choose which sources they want (YouTube, OpenAI, Anthropic, F1)
- **Content Filtering**: Options to filter by topics, keywords, or categories
- **Delivery Frequency**: Choose daily, weekly, or custom schedules
- **Personalization**: AI-powered content recommendations based on reading preferences
- Unsubscribe management and email preferences

---

## ğŸ¤ Contributing

This is a personal project, but suggestions and improvements are welcome!

---

## ğŸ“„ License

This project is open source and available for educational purposes.

---

## ğŸ™ Acknowledgments

- **Google Gemini** for AI summarization
- **OpenAI, Anthropic** for providing RSS feeds
- **YouTube** for video content
- **PostgreSQL** for reliable data storage

---

**Built with â¤ï¸ using Python and AI**
